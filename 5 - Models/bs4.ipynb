{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is the process of extracting data from websites. Beautiful Soup is a powerful Python library that makes parsing HTML and XML documents easy.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yassine\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yassine\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yassine\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yassine\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.5 MB 399.0 kB/s eta 0:00:28\n",
      "   - -------------------------------------- 0.5/11.5 MB 399.0 kB/s eta 0:00:28\n",
      "   - -------------------------------------- 0.5/11.5 MB 399.0 kB/s eta 0:00:28\n",
      "   - -------------------------------------- 0.5/11.5 MB 399.0 kB/s eta 0:00:28\n",
      "   - -------------------------------------- 0.5/11.5 MB 399.0 kB/s eta 0:00:28\n",
      "   - -------------------------------------- 0.5/11.5 MB 399.0 kB/s eta 0:00:28\n",
      "   -- ------------------------------------- 0.8/11.5 MB 266.3 kB/s eta 0:00:41\n",
      "   -- ------------------------------------- 0.8/11.5 MB 266.3 kB/s eta 0:00:41\n",
      "   -- ------------------------------------- 0.8/11.5 MB 266.3 kB/s eta 0:00:41\n",
      "   -- ------------------------------------- 0.8/11.5 MB 266.3 kB/s eta 0:00:41\n",
      "   -- ------------------------------------- 0.8/11.5 MB 266.3 kB/s eta 0:00:41\n",
      "   -- ------------------------------------- 0.8/11.5 MB 266.3 kB/s eta 0:00:41\n",
      "   --- ------------------------------------ 1.0/11.5 MB 236.3 kB/s eta 0:00:45\n",
      "   --- ------------------------------------ 1.0/11.5 MB 236.3 kB/s eta 0:00:45\n",
      "   --- ------------------------------------ 1.0/11.5 MB 236.3 kB/s eta 0:00:45\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 262.1 kB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 262.1 kB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 262.1 kB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 278.7 kB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 278.7 kB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 278.7 kB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 278.7 kB/s eta 0:00:36\n",
      "   ------ --------------------------------- 1.8/11.5 MB 285.1 kB/s eta 0:00:34\n",
      "   ------ --------------------------------- 1.8/11.5 MB 285.1 kB/s eta 0:00:34\n",
      "   ------ --------------------------------- 1.8/11.5 MB 285.1 kB/s eta 0:00:34\n",
      "   ------ --------------------------------- 1.8/11.5 MB 285.1 kB/s eta 0:00:34\n",
      "   ------- -------------------------------- 2.1/11.5 MB 285.7 kB/s eta 0:00:33\n",
      "   ------- -------------------------------- 2.1/11.5 MB 285.7 kB/s eta 0:00:33\n",
      "   ------- -------------------------------- 2.1/11.5 MB 285.7 kB/s eta 0:00:33\n",
      "   -------- ------------------------------- 2.4/11.5 MB 293.7 kB/s eta 0:00:32\n",
      "   -------- ------------------------------- 2.4/11.5 MB 293.7 kB/s eta 0:00:32\n",
      "   -------- ------------------------------- 2.4/11.5 MB 293.7 kB/s eta 0:00:32\n",
      "   -------- ------------------------------- 2.4/11.5 MB 293.7 kB/s eta 0:00:32\n",
      "   -------- ------------------------------- 2.4/11.5 MB 293.7 kB/s eta 0:00:32\n",
      "   --------- ------------------------------ 2.6/11.5 MB 283.8 kB/s eta 0:00:32\n",
      "   --------- ------------------------------ 2.6/11.5 MB 283.8 kB/s eta 0:00:32\n",
      "   --------- ------------------------------ 2.6/11.5 MB 283.8 kB/s eta 0:00:32\n",
      "   --------- ------------------------------ 2.6/11.5 MB 283.8 kB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 288.8 kB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 288.8 kB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 298.6 kB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 298.6 kB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 298.6 kB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 298.6 kB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 303.6 kB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 303.6 kB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 303.6 kB/s eta 0:00:27\n",
      "   ------------ --------------------------- 3.7/11.5 MB 308.0 kB/s eta 0:00:26\n",
      "   ------------ --------------------------- 3.7/11.5 MB 308.0 kB/s eta 0:00:26\n",
      "   ------------ --------------------------- 3.7/11.5 MB 308.0 kB/s eta 0:00:26\n",
      "   ------------- -------------------------- 3.9/11.5 MB 310.3 kB/s eta 0:00:25\n",
      "   ------------- -------------------------- 3.9/11.5 MB 310.3 kB/s eta 0:00:25\n",
      "   ------------- -------------------------- 3.9/11.5 MB 310.3 kB/s eta 0:00:25\n",
      "   ------------- -------------------------- 3.9/11.5 MB 310.3 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 4.2/11.5 MB 313.0 kB/s eta 0:00:24\n",
      "   -------------- ------------------------- 4.2/11.5 MB 313.0 kB/s eta 0:00:24\n",
      "   -------------- ------------------------- 4.2/11.5 MB 313.0 kB/s eta 0:00:24\n",
      "   --------------- ------------------------ 4.5/11.5 MB 316.9 kB/s eta 0:00:23\n",
      "   --------------- ------------------------ 4.5/11.5 MB 316.9 kB/s eta 0:00:23\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 321.5 kB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 321.5 kB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 321.5 kB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 321.5 kB/s eta 0:00:22\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 319.9 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 319.9 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 319.9 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 319.9 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 319.9 kB/s eta 0:00:21\n",
      "   ------------------ --------------------- 5.2/11.5 MB 316.2 kB/s eta 0:00:20\n",
      "   ------------------ --------------------- 5.2/11.5 MB 316.2 kB/s eta 0:00:20\n",
      "   ------------------ --------------------- 5.2/11.5 MB 316.2 kB/s eta 0:00:20\n",
      "   ------------------- -------------------- 5.5/11.5 MB 318.0 kB/s eta 0:00:19\n",
      "   ------------------- -------------------- 5.5/11.5 MB 318.0 kB/s eta 0:00:19\n",
      "   -------------------- ------------------- 5.8/11.5 MB 323.5 kB/s eta 0:00:18\n",
      "   -------------------- ------------------- 5.8/11.5 MB 323.5 kB/s eta 0:00:18\n",
      "   -------------------- ------------------- 5.8/11.5 MB 323.5 kB/s eta 0:00:18\n",
      "   -------------------- ------------------- 5.8/11.5 MB 323.5 kB/s eta 0:00:18\n",
      "   -------------------- ------------------- 6.0/11.5 MB 323.2 kB/s eta 0:00:17\n",
      "   -------------------- ------------------- 6.0/11.5 MB 323.2 kB/s eta 0:00:17\n",
      "   -------------------- ------------------- 6.0/11.5 MB 323.2 kB/s eta 0:00:17\n",
      "   --------------------- ------------------ 6.3/11.5 MB 326.2 kB/s eta 0:00:16\n",
      "   --------------------- ------------------ 6.3/11.5 MB 326.2 kB/s eta 0:00:16\n",
      "   --------------------- ------------------ 6.3/11.5 MB 326.2 kB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 328.2 kB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 328.2 kB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 328.2 kB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 330.0 kB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 330.0 kB/s eta 0:00:15\n",
      "   ------------------------ --------------- 7.1/11.5 MB 334.8 kB/s eta 0:00:14\n",
      "   ------------------------ --------------- 7.1/11.5 MB 334.8 kB/s eta 0:00:14\n",
      "   ------------------------ --------------- 7.1/11.5 MB 334.8 kB/s eta 0:00:14\n",
      "   ------------------------- -------------- 7.3/11.5 MB 339.1 kB/s eta 0:00:13\n",
      "   ------------------------- -------------- 7.3/11.5 MB 339.1 kB/s eta 0:00:13\n",
      "   -------------------------- ------------- 7.6/11.5 MB 343.6 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 7.6/11.5 MB 343.6 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 7.9/11.5 MB 348.3 kB/s eta 0:00:11\n",
      "   --------------------------- ------------ 7.9/11.5 MB 348.3 kB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 351.2 kB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 351.2 kB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 351.2 kB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 352.8 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 352.8 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 352.8 kB/s eta 0:00:09\n",
      "   ------------------------------ --------- 8.7/11.5 MB 353.2 kB/s eta 0:00:09\n",
      "   ------------------------------ --------- 8.7/11.5 MB 353.2 kB/s eta 0:00:09\n",
      "   ------------------------------ --------- 8.7/11.5 MB 353.2 kB/s eta 0:00:09\n",
      "   ------------------------------ --------- 8.9/11.5 MB 354.2 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 8.9/11.5 MB 354.2 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 8.9/11.5 MB 354.2 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 9.2/11.5 MB 355.8 kB/s eta 0:00:07\n",
      "   ------------------------------- -------- 9.2/11.5 MB 355.8 kB/s eta 0:00:07\n",
      "   ------------------------------- -------- 9.2/11.5 MB 355.8 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 9.4/11.5 MB 357.4 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 9.4/11.5 MB 357.4 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 9.4/11.5 MB 357.4 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 9.7/11.5 MB 358.2 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 9.7/11.5 MB 358.2 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 9.7/11.5 MB 358.2 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 359.9 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 359.9 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 359.9 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 359.9 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 357.2 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 357.2 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 357.2 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 10.5/11.5 MB 357.9 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 10.5/11.5 MB 357.9 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 10.5/11.5 MB 357.9 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 10.7/11.5 MB 357.7 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 10.7/11.5 MB 357.7 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 10.7/11.5 MB 357.7 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 10.7/11.5 MB 357.7 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 11.0/11.5 MB 355.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 11.0/11.5 MB 355.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 11.0/11.5 MB 355.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 11.0/11.5 MB 355.1 kB/s eta 0:00:02\n",
      "   ---------------------------------------  11.3/11.5 MB 361.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 361.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 361.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 361.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 358.6 kB/s eta 0:00:00\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: pytz, urllib3, soupsieve, idna, charset-normalizer, certifi, requests, pandas, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 pandas-2.2.3 pytz-2024.2 requests-2.32.3 soupsieve-2.6 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4 requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Web Scraping Example: Scraping Jumia Morocco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping Tutorial: Mastering BeautifulSoup for Data Extraction\n",
    "\n",
    "## Introduction to Web Scraping with BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# First, let's ensure we have the necessary libraries installed\n",
    "# You can install them using:\n",
    "# pip install requests beautifulsoup4 pandas\n",
    "\n",
    "### Understanding HTML Structure and BeautifulSoup Basics\n",
    "\n",
    "def explain_html_parsing():\n",
    "    \"\"\"\n",
    "    Demonstration of BeautifulSoup's core parsing capabilities\n",
    "    \"\"\"\n",
    "    # Sample HTML to demonstrate parsing techniques\n",
    "    sample_html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <div class=\"product\" id=\"item1\">\n",
    "                <h2 class=\"product-title\">Smartphone X</h2>\n",
    "                <p class=\"price\">$499.99</p>\n",
    "                <span class=\"brand\">TechBrand</span>\n",
    "            </div>\n",
    "            <div class=\"product\" id=\"item2\">\n",
    "                <h2 class=\"product-title\">Laptop Pro</h2>\n",
    "                <p class=\"price\">$899.99</p>\n",
    "                <span class=\"brand\">ComputerCo</span>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create BeautifulSoup object\n",
    "\n",
    "    soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "    \n",
    "    # Finding elements by tag\n",
    "    all_divs = soup.find_all('div')\n",
    "    print(\"All div elements:\", [div.get('id') for div in all_divs])\n",
    "    \n",
    "    # Finding elements by class\n",
    "    product_titles = soup.find_all(class_='product-title')\n",
    "    product_titles\n",
    "    print(\"Product Titles:\", [title.text for title in product_titles])\n",
    "    \n",
    "    # Finding elements by ID\n",
    "    specific_product = soup.find(id='item1')\n",
    "    print(\"Specific Product Title:\", specific_product.find('h2').text)\n",
    "    \n",
    "    '''# Nested searching\n",
    "    prices = [div.find('p', class_='price').text for div in soup.find_all('div', class_='product')]\n",
    "    print(\"Product Prices:\", prices)'''\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Web Scraping Example: Scraping Jumia Morocco\n",
    "\n",
    "def scrape_jumia_products():\n",
    "    \"\"\"\n",
    "    Web scraping example from Jumia Morocco\n",
    "    Demonstrates real-world web scraping techniques\n",
    "    \"\"\"\n",
    "    # Headers to mimic browser request and avoid potential blocking\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'\n",
    "    }\n",
    "    \n",
    "    # URL of Jumia Morocco's Electronics category\n",
    "    url = 'https://www.jumia.ma/catalog/?q=phones-tablets'\n",
    "    \n",
    "    try:\n",
    "        # Send GET request\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Create BeautifulSoup object\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract product information\n",
    "        products = []\n",
    "        \n",
    "        # Find all product articles\n",
    "        product_articles = soup.find_all('article', class_='prd _fb col c-prd')\n",
    "        \n",
    "        for product in product_articles:\n",
    "            try:\n",
    "                # Extract product details\n",
    "                title = product.find('h3', class_='name').text.strip()\n",
    "                \n",
    "                # Find price (handling potential missing price)\n",
    "                price_elem = product.find('div', class_='prc')\n",
    "                price = price_elem.text.strip() if price_elem else 'Price Not Available'\n",
    "                \n",
    "                # Find brand (if available)\n",
    "                brand_elem = product.find('div', class_='brand')\n",
    "                brand = brand_elem.text.strip() if brand_elem else 'Unknown Brand'\n",
    "                \n",
    "                # Extract product link\n",
    "                link_elem = product.find('a', class_='core')\n",
    "                product_link = link_elem['href'] if link_elem else 'No Link'\n",
    "                \n",
    "                products.append({\n",
    "                    'Title': title,\n",
    "                    'Price': price,\n",
    "                    'Brand': brand,\n",
    "                    'Link': product_link\n",
    "                })\n",
    "            \n",
    "            except Exception as detail_error:\n",
    "                print(f\"Error parsing individual product: {detail_error}\")\n",
    "        \n",
    "        # Convert to DataFrame for easy analysis\n",
    "        df = pd.DataFrame(products)\n",
    "        return df\n",
    "    \n",
    "    except requests.RequestException as req_error:\n",
    "        print(f\"Request error occurred: {req_error}\")\n",
    "        return None\n",
    "\n",
    "# Advanced Parsing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_parsing_techniques():\n",
    "    \"\"\"\n",
    "    Demonstrates advanced BeautifulSoup parsing methods\n",
    "    \"\"\"\n",
    "    # Sample complex HTML\n",
    "    complex_html = \"\"\"\n",
    "    <div class=\"catalog\">\n",
    "        <section class=\"products\">\n",
    "            <article data-category=\"electronics\">\n",
    "                <h2>Smart Watch</h2>\n",
    "                <ul class=\"specs\">\n",
    "                    <li>Color: Black</li>\n",
    "                    <li>Battery: 500mAh</li>\n",
    "                </ul>\n",
    "            </article>\n",
    "            <article data-category=\"computers\">\n",
    "                <h2>Gaming Laptop</h2>\n",
    "                <ul class=\"specs\">\n",
    "                    <li>RAM: 16GB</li>\n",
    "                    <li>Processor: Intel i7</li>\n",
    "                </ul>\n",
    "            </article>\n",
    "        </section>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(complex_html, 'html.parser')\n",
    "    \n",
    "    # CSS Selector usage\n",
    "    electronics = soup.select('article[data-category=\"electronics\"]')\n",
    "    computers = soup.select('article[data-category=\"computers\"]')\n",
    "    \n",
    "    print(\"Electronics Products:\", [elem.find('h2').text for elem in electronics])\n",
    "    \n",
    "    # Extracting nested information\n",
    "    def extract_specs(article):\n",
    "        return {\n",
    "            'Name': article.find('h2').text,\n",
    "            'Specs': [spec.text for spec in article.find_all('li')]\n",
    "        }\n",
    "    \n",
    "    all_products = [extract_specs(article) for article in soup.find_all('article')]\n",
    "    print(\"Detailed Products:\", all_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All div elements: ['item1', 'item2']\n",
      "Product Titles: ['Smartphone X', 'Laptop Pro']\n",
      "Specific Product Title: Smartphone X\n",
      "Product Prices: ['$499.99', '$899.99']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Handling Common Web Scraping Challenges\n",
    "\n",
    "def web_scraping_best_practices():\n",
    "    \"\"\"\n",
    "    Best practices and error handling in web scraping\n",
    "    \"\"\"\n",
    "    # 1. Respect robots.txt\n",
    "    # 2. Add delays between requests\n",
    "    # 3. Use proper error handling\n",
    "    # 4. Rotate User-Agents\n",
    "    # 5. Handle various HTML structures gracefully\n",
    "    \n",
    "    print(\"Web Scraping Best Practices Demonstrated in Code\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == '__main__':\n",
    "    # Demonstrate different techniques\n",
    "    explain_html_parsing()\n",
    "    \n",
    "    # Uncomment to run Jumia scraping (be mindful of website's terms of service)\n",
    "    # jumia_products = scrape_jumia_products()\n",
    "    # print(jumia_products)\n",
    "    \n",
    "   \n",
    "    #advanced_parsing_techniques()\n",
    "    #web_scraping_best_practices()\n",
    "\n",
    "# Note: Always check website's robots.txt and terms of service before scraping\n",
    "# Ensure you have permission or are complying with legal and ethical guidelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
